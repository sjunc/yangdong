# =============================================================================
# app.py
# -----------------------------------------------------------------------------
# ğŸ™ï¸ STT/TTS + GPT-4o-mini í†µí•© FastAPI ì„œë²„
# -----------------------------------------------------------------------------
# - STT: faster-whisper (CPU ê¸°ë³¸)
# - TTS: edge-tts (MP3 base64 ë°˜í™˜)
# - LLM: GPT-4o-mini (ai/llm_runtime/llm_client.py)
# - Guard: ìš•ì„¤/PII ìë™ í•„í„°ë§ (stt-tts-sample/guard.py)
#
# ì‹¤í–‰:
#   uvicorn app:app --reload --port 9000
#
# ì˜ì¡´ì„±:
#   pip install fastapi uvicorn[standard] faster-whisper edge-tts python-dotenv
#   (Windows) FFmpeg ê¶Œì¥: winget install Gyan.FFmpeg
# =============================================================================

import os
import base64
import asyncio
from io import BytesIO
import time
from typing import Optional, List, Dict, Any

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, FileResponse
from pydantic import BaseModel

from faster_whisper import WhisperModel
import edge_tts
from dotenv import load_dotenv

from fastapi.staticfiles import StaticFiles

# âœ… ìƒˆ GPT LLM í´ë¼ì´ì–¸íŠ¸ + Guard ëª¨ë“ˆ
import sys, os

# add parent folder (ai/) to sys.path so we can import llm_runtime/*
CURRENT_DIR = os.path.dirname(__file__)
PARENT_AI_DIR = os.path.abspath(os.path.join(CURRENT_DIR, ".."))
if PARENT_AI_DIR not in sys.path:
    sys.path.insert(0, PARENT_AI_DIR)

# add project root to sys.path so we can import from frontend/
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from llm_runtime.llm_client import chat
from guard import violates_policy

# -----------------------------------------------------------------------------
# Windowsìš© ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì • (asyncio ê´€ë ¨ ì˜¤ë¥˜ ë°©ì§€)
# -----------------------------------------------------------------------------
import sys
if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# -----------------------------------------------------------------------------
# .env ë¡œë“œ (ê²½ë¡œ ëª…ì‹œ)
# -----------------------------------------------------------------------------
# LLM ì„¤ì •ê³¼ RAG/ì„œë²„ ì„¤ì •ì„ ê°ê¸° ë‹¤ë¥¸ .env íŒŒì¼ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œ
LLM_RUNTIME_ENV_PATH = os.path.join(PARENT_AI_DIR, "llm_runtime", ".env")
STT_TTS_ENV_PATH = os.path.join(CURRENT_DIR, ".env")

# .env íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ë¡œë“œ
if os.path.exists(LLM_RUNTIME_ENV_PATH):
    load_dotenv(dotenv_path=LLM_RUNTIME_ENV_PATH)
    print(f"Loaded .env from: {LLM_RUNTIME_ENV_PATH}")

if os.path.exists(STT_TTS_ENV_PATH):
    load_dotenv(dotenv_path=STT_TTS_ENV_PATH, override=True)
    print(f"Loaded .env from: {STT_TTS_ENV_PATH}")

# -----------------------------------------------------------------------------
# Config (STT/TTSë§Œ ìœ ì§€)
# -----------------------------------------------------------------------------
# whisper: small/medium ë“± ê°€ëŠ¥ (CPU ê¸°ë³¸)
WHISPER_MODEL_SIZE = os.getenv("WHISPER_MODEL_SIZE", "small")
# cpu / cuda / auto(ë¯¸ì§€ì› ì‹œ cpuë¡œ í´ë°±)
WHISPER_DEVICE = (os.getenv("WHISPER_DEVICE", "cpu") or "cpu").lower()

# edge-tts í•œêµ­ì–´ ìŒì„± & í¬ë§· (MP3 ê¶Œì¥)
TTS_VOICE = os.getenv("TTS_VOICE", "ko-KR-SunHiNeural")

# -----------------------------------------------------------------------------
# FastAPI ì•± ì„¤ì •
# -----------------------------------------------------------------------------
app = FastAPI(title="STT/TTS + GPT-4o-mini", version="1.2.0")

# -----------------------------------------------------------------------------
# âœ… CORS ì„¤ì • (í”„ë¡ íŠ¸ ì—°ê²°ìš©)
# -----------------------------------------------------------------------------
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://127.0.0.1:8001",   # í”„ë¡ íŠ¸ ì£¼ì†Œ
        "http://localhost:8001",   # ì¼ë¶€ ë¸Œë¼ìš°ì €ìš©
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# The /static and / routes are now handled by the mounted Flask app.

# -----------------------------------------------------------------------------
# ë¹„ë™ê¸° warmup
# -----------------------------------------------------------------------------
WARMUP_ON_STARTUP = os.getenv("WARMUP_ON_STARTUP", "true").lower() == "true"

warmup_state = {
    "running": False,
    "done": False,
    "started_at": None,
    "finished_at": None,
    "steps": [],   # ê° ë‹¨ê³„ ë¡œê·¸
}

def _step(msg: str):
    warmup_state["steps"].append({"t": time.strftime("%H:%M:%S"), "msg": msg})

# app.py - warmup ë‚´ë¶€ë¥¼ ê²½ëŸ‰í™”(ì¸ë±ìŠ¤ í™•ì¸ ì œê±°)
async def _warmup():
    if warmup_state["running"] or warmup_state["done"]:
        return
    warmup_state.update({"running": True, "done": False, "started_at": time.time(), "steps": []})
    try:
        # LLM ping
        try:
            _step("LLM ping")
            _ = llm_chat(messages=[{"role": "user", "content": "ping"}], temperature=0.0, max_tokens=1)
            _step("LLM ping ok")
        except Exception as e:
            _step(f"LLM ping failed: {e}")

        _step("all done")
    except Exception as e:
        _step(f"warmup error: {type(e).__name__}: {e}")
    finally:
        warmup_state.update({"running": False, "done": True, "finished_at": time.time()})


@app.on_event("startup")
async def on_startup():
    if WARMUP_ON_STARTUP:
        asyncio.create_task(_warmup())

# --- ìƒíƒœ í™•ì¸/ìˆ˜ë™ ì‹œì‘ ---
@app.get("/warmup/status")
def warmup_status():
    s = dict(warmup_state)
    if s["started_at"] is not None:
        s["started_at"] = int(s["started_at"])
    if s["finished_at"] is not None:
        s["finished_at"] = int(s["finished_at"])
    return s

@app.post("/warmup/start")
async def warmup_start():
    asyncio.create_task(_warmup())
    return {"ok": True, "started": True}

# -----------------------------------------------------------------------------
# Lazy-loaded STT ëª¨ë¸
# -----------------------------------------------------------------------------
_whisper_model: Optional[WhisperModel] = None

def _compute_type_for(device: str) -> str:
    """GPUë©´ float16, ê·¸ ì™¸ì—ëŠ” int8ë¡œ ê²½ëŸ‰í™”."""
    return "float16" if device == "cuda" else "int8"

def _normalize_device(device: str) -> str:
    return device if device in {"cpu", "cuda"} else "cpu"

def get_stt_model() -> WhisperModel:
    """Whisper ëª¨ë¸ì„ ì „ì—­ ì‹±ê¸€í†¤ìœ¼ë¡œ ë¡œë“œ"""
    global _whisper_model
    if _whisper_model is None:
        device = _normalize_device(WHISPER_DEVICE)
        _whisper_model = WhisperModel(
            model_size_or_path=WHISPER_MODEL_SIZE,
            device=device,
            compute_type=_compute_type_for(device),
        )
    return _whisper_model

# -----------------------------------------------------------------------------
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
# -----------------------------------------------------------------------------
class ChatRequest(BaseModel):
    text: str

class TTSRequest(BaseModel):
    text: str
    voice: Optional[str] = None

class VoiceChatResponse(BaseModel):
    user_text: str
    assistant_text: str
    audio_b64: str       # MP3 Base64
    audio_mime: str = "audio/mpeg"

# -----------------------------------------------------------------------------
# Helper í•¨ìˆ˜ë“¤
# -----------------------------------------------------------------------------
def stt_transcribe_bytes(audio_bytes: bytes) -> Dict[str, Any]:
    """Bytes â†’ STT â†’ {text, language, segments}"""
    if not audio_bytes:
        raise ValueError("empty audio payload")

    model = get_stt_model()
    segments, info = model.transcribe(BytesIO(audio_bytes), beam_size=1)

    out_segments, full_text_parts = [], []
    for seg in segments:
        seg_text = (seg.text or "").strip()
        if seg_text:
            full_text_parts.append(seg_text)
            out_segments.append(
                {"text": seg_text, "start": float(seg.start), "end": float(seg.end)}
            )

    text = " ".join(full_text_parts).strip()
    language = info.language or "unknown"
    return {"text": text, "language": language, "segments": out_segments}

def chat_answer(user_text: str) -> str:
    """
    í…ìŠ¤íŠ¸ â†’ GPT-4o-mini ë‹µë³€(str)
    (ê¸°ì¡´ TinyLlama/vLLM í˜¸ì¶œì„ llm_runtime.llm_client.chat()ìœ¼ë¡œ êµì²´)
    """
    # ğŸ›¡ï¸ Guard: ê¸ˆì§€ì–´/PII í¬í•¨ ì‹œ ì°¨ë‹¨ ë©˜íŠ¸
    if violates_policy(user_text):
        return "âš ï¸ ë¶€ì ì ˆí•˜ê±°ë‚˜ ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ìš”ì²­ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ ì£¼ì„¸ìš”."

    messages = [
        {"role": "system", "content": "You are a helpful Korean assistant."},
        {"role": "user", "content": user_text.strip()},
    ]
    return chat(messages)  # <-- GPT-4o-mini í˜¸ì¶œ

async def tts_synthesize_mp3(text: str, voice: str) -> bytes:
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±(MP3)ìœ¼ë¡œ ë³€í™˜"""
    try:
        communicate = edge_tts.Communicate(text=text, voice=voice)
        buf = BytesIO()
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                buf.write(chunk["data"])
        data = buf.getvalue()
        if not data:
            raise RuntimeError("edge-tts produced empty audio")
        return data
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"TTS synth failed: {e}")

# -----------------------------------------------------------------------------
# Health ì²´í¬
# -----------------------------------------------------------------------------
@app.get("/health")
def health():
    return {
        "status": "ok",
        "llm": "gpt-4o-mini-via-llm_runtime",
        "warmup_done": warmup_state["done"],
        "warmup_running": warmup_state["running"],
    }

# -----------------------------------------------------------------------------
# STT ì—”ë“œí¬ì¸íŠ¸
# -----------------------------------------------------------------------------
@app.post("/stt")
async def stt_endpoint(file: UploadFile = File(...)):
    """ì—…ë¡œë“œëœ ìŒì„± íŒŒì¼ì„ STT ì²˜ë¦¬"""
    if not file.filename:
        raise HTTPException(status_code=400, detail="No file uploaded")

    try:
        audio = await file.read()
        result = stt_transcribe_bytes(audio)
        return JSONResponse(result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"STT error: {e}")

# -----------------------------------------------------------------------------
# Chat ì—”ë“œí¬ì¸íŠ¸
# -----------------------------------------------------------------------------
@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    """í…ìŠ¤íŠ¸ ì…ë ¥ â†’ GPT-4o-mini ì‘ë‹µ ë°˜í™˜"""
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")
    try:
        answer = chat_answer(text)
        return {"answer": answer}
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"LLM request failed: {e}")

# -----------------------------------------------------------------------------
# TTS ì—”ë“œí¬ì¸íŠ¸
# -----------------------------------------------------------------------------
@app.post("/tts")
async def tts_endpoint(req: TTSRequest):
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±(MP3 base64)ìœ¼ë¡œ ë³€í™˜"""
    text = (req.text or "").strip()
    if not text:
        raise HTTPException(status_code=400, detail="Empty text")

    voice = (req.voice or TTS_VOICE).strip()
    try:
        mp3_bytes = await tts_synthesize_mp3(text, voice)
        audio_b64 = base64.b64encode(mp3_bytes).decode("utf-8")
        return {"audio_b64": audio_b64, "mime": "audio/mpeg"}
    except HTTPException as e:
        raise e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"TTS error: {e}")

# -----------------------------------------------------------------------------
# Voice Chat ì—”ë“œí¬ì¸íŠ¸
# -----------------------------------------------------------------------------
@app.post("/voice-chat", response_model=VoiceChatResponse)
async def voice_chat(file: UploadFile = File(...)):
    """ìŒì„± â†’ STT â†’ GPT-4o-mini â†’ TTS â†’ JSON ë°˜í™˜"""
    # 1) STT
    try:
        audio = await file.read()
        stt = stt_transcribe_bytes(audio)
        user_text = (stt.get("text") or "").strip()
        if not user_text:
            raise HTTPException(status_code=400, detail="STT produced empty text")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"voice-chat STT error: {e}")

    # 2) LLM (GPT-4o-mini)
    try:
        assistant_text = chat_answer(user_text) or "ì£„ì†¡í•´ìš”. ì§€ê¸ˆì€ ëŒ€ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ìš”."
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"voice-chat LLM error: {e}")

    # 3) TTS
    try:
        mp3_bytes = await tts_synthesize_mp3(assistant_text, TTS_VOICE)
        audio_b64 = base64.b64encode(mp3_bytes).decode("utf-8")
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"voice-chat TTS error: {e}")

    return VoiceChatResponse(
        user_text=user_text,    
        assistant_text=assistant_text,
        audio_b64=audio_b64,
        audio_mime="audio/mpeg",
    )

# -----------------------------------------------------------------------------
# RAG - Mounted App
# -----------------------------------------------------------------------------
# All RAG logic is now handled by the self-contained FastAPI app in /rag/app.py
# It is mounted under the /rag prefix.
# -----------------------------------------------------------------------------
from rag.app import app as rag_app

app.mount("/rag", rag_app, name="rag")


# -----------------------------------------------------------------------------
# Frontend - Mount Flask App as main UI
# -----------------------------------------------------------------------------
from frontend.app import app as flask_app
from asgiref.wsgi import WsgiToAsgi

# Mount the Flask app at the root. This will handle all UI routes.
app.mount("/", WsgiToAsgi(flask_app), name="frontend")


# -----------------------------------------------------------------------------
# Standalone LLM Ping (for testing)
# -----------------------------------------------------------------------------
from typing import Optional, Dict, List
from pydantic import BaseModel
from fastapi import HTTPException
from llm_runtime.llm_client import chat as llm_chat
from rag.refactored_rag import ingest_data, ask_question

# --- Old RAG imports (to be removed or replaced) ---
# from rag.auto_index import ensure_index_ready
# from rag.ingest import ingest_all, embedder
# from rag.retriever import retrieve
# from rag.qa import answer as rag_answer

class IngestAllReq(BaseModel):
    pdf_paths: Optional[List[str]] = None
    mongo_query: Optional[Dict] = None

@app.post("/rag/ingest")
def rag_ingest(req: Optional[IngestAllReq] = None):
    try:
        ingest_data(pdf_paths=req.pdf_paths if req else None,
                    mongo_query=req.mongo_query if req else None)
        return {"status":"ok", "message": "Ingestion complete."}
    except Exception as e:
        raise HTTPException(500, f"Ingest failed: {e}")

class RagChatReq(BaseModel):
    query: str
    top_k: int = 6
    # dataset ë“± í•„í„°: {"dataset": ["ê²½ì˜í•™ê³¼","ì „ê¸°ê³µí•™ê³¼"]}
    filters: Optional[Dict[str, List[str]]] = None

@app.post("/rag/chat")
def rag_chat(req: RagChatReq):
    q = (req.query or "").strip()
    if not q:
        raise HTTPException(status_code=400, detail="Empty query")

    t0 = time.perf_counter()

    try:
        result = ask_question(query=q, k=req.top_k, filters=req.filters)

        latency_ms = int((time.perf_counter() - t0) * 1000)
        return {
            "answer": result["answer"],
            "sources": result["sources"],
            "latency_ms": latency_ms,   # ë””ë²„ê¹…ìš© ì§€ì—° ì‹œê°„
        }

    except TimeoutError:
        raise HTTPException(status_code=504, detail="LLM timeout")
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"RAG failed: {e}")

@app.post("/rag/preview")
def rag_preview(req: RagChatReq):
    try:
        result = ask_question(query=req.query, k=req.top_k, filters=req.filters)
        
        chunks_output = []
        for source in result["sources"]:
            chunks_output.append({
                "page": source.get("page"),
                "source_type": source.get("source_type"),
                "title": source.get("title"),
                "dataset": source.get("dataset"),
                "score": source.get("score", 0), # ask_question currently returns None for score
                "text": source.get("text", "")[:500]
            })
        
        return {"chunks": chunks_output}
    except Exception as e:
        raise HTTPException(500, f"Preview failed: {e}")

# ---------- Mongo ë””ë²„ê·¸ ----------
from pymongo import MongoClient
from fastapi import APIRouter

@app.get("/rag/debug/mongo")
def rag_debug_mongo():
    import os, traceback
    from rag.config import MONGO_URI, MONGO_DB, MONGO_COLL, MONGO_UPDATED_FIELD
    out = {"ok": False, "uri": MONGO_URI, "db": MONGO_DB, "coll": MONGO_COLL, "updated_field": MONGO_UPDATED_FIELD}
    try:
        cli = MongoClient(MONGO_URI)
        db  = cli[MONGO_DB]
        colls = [c for c in db.list_collection_names() if not c.startswith("system.")]
        out["collections"] = colls
        samples = {}
        for name in colls[:5]:
            doc = db[name].find_one()
            samples[name] = {k: doc.get(k) for k in ["title","subject","content","body","summary","text","updated_at"]} if doc else None
        out["samples"] = samples
        out["ok"] = True
    except Exception as e:
        out["error"] = f"{type(e).__name__}: {e}"
        out["trace"] = traceback.format_exc(limit=3)
    return out

# -- ì„ì‹œ í…ŒìŠ¤íŠ¸
@app.get("/llm/ping")
def llm_ping():
    try:
        msg = [{"role":"user","content":"ping"}]
        txt = llm_chat(messages=msg, temperature=0.0, max_tokens=4)
        return {"ok": True, "model": "OPENAI_MODEL from llm_runtime/.env", "answer": txt}
    except Exception as e:
        import traceback
        return {"ok": False, "error": str(e), "trace": traceback.format_exc(limit=2)}
